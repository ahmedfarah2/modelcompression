{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKDclGfMEyJV",
        "outputId": "fa34f9ec-44a6-4cc0-d6da-0349639b8048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1200\n",
            "Validation set size: 300\n",
            "Test set size: 500\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "import os\n",
        "import time\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load IMDB dataset and create subsets\n",
        "dataset = load_dataset('imdb')\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in range(1500)])\n",
        "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in range(500)])\n",
        "\n",
        "# Split the training dataset into train and validation sets\n",
        "train_val_split = small_train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_val_split[\"train\"]\n",
        "validation_dataset = train_val_split[\"test\"]\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(validation_dataset)}\")\n",
        "print(f\"Test set size: {len(small_test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and tokenize datasets\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
        "small_test_dataset = small_test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "small_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Tokenization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElF-67ZGE5qc",
        "outputId": "9ff4cdeb-1292-4159-efc8-7152529b3d01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yODpv-xE9oB",
        "outputId": "1be026f5-2fbb-4e06-d3b0-16d1d462bf0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded with 109,483,778 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import prune\n",
        "\n",
        "def calculate_pruning_impact(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_pruned_weights = 0\n",
        "\n",
        "    for transformer_layer in model.bert.encoder.layer:\n",
        "        weights = transformer_layer.intermediate.dense.weight.detach().cpu()\n",
        "        total_pruned_weights += (weights == 0).sum().item()\n",
        "\n",
        "    remaining_params = total_params - total_pruned_weights\n",
        "    percent_pruned = (total_pruned_weights / total_params) * 100\n",
        "    print(f\"Total Parameters: {total_params:,}, Pruned Parameters: {total_pruned_weights:,}, Pruned Percentage: {percent_pruned:.2f}%\")\n",
        "    return total_params, total_pruned_weights, remaining_params, percent_pruned\n",
        "\n",
        "def apply_structured_pruning(layer, amount=0.2):\n",
        "    prune.ln_structured(layer, name=\"weight\", amount=amount, n=1, dim=0)"
      ],
      "metadata": {
        "id": "AOwFO1kPE_w-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating original model parameters...\")\n",
        "original_model_memory, _, _, _ = calculate_pruning_impact(model)\n",
        "\n",
        "# Apply structured pruning to all layers\n",
        "for i, transformer_layer in enumerate(model.bert.encoder.layer):\n",
        "    apply_structured_pruning(transformer_layer.intermediate.dense, amount=0.2)\n",
        "    apply_structured_pruning(transformer_layer.output.dense, amount=0.2)\n",
        "\n",
        "print(\"Calculating pruned model parameters...\")\n",
        "_, _, pruned_model_memory, _ = calculate_pruning_impact(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyfkYn3RFBIb",
        "outputId": "6784dcfa-758e-42f1-86dc-8b5d5c8ccf16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating original model parameters...\n",
            "Total Parameters: 109,483,778, Pruned Parameters: 0, Pruned Percentage: 0.00%\n",
            "Calculating pruned model parameters...\n",
            "Total Parameters: 109,483,778, Pruned Parameters: 5,658,624, Pruned Percentage: 5.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "QA6OqMgzJByE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./structured_pruned_bert_output\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./structured_pruned_logs\",\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(\"Training structured pruned model...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "o6cRenxDFEDm",
        "outputId": "23528baf-8319-407e-d7c8-9452154b93dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-7-222b65caec65>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training structured pruned model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1500 07:38 < 01:54, 2.61 it/s, Epoch 8/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.703800</td>\n",
              "      <td>0.692490</td>\n",
              "      <td>0.603333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.698900</td>\n",
              "      <td>0.726501</td>\n",
              "      <td>0.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.605552</td>\n",
              "      <td>0.673333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.589900</td>\n",
              "      <td>0.679820</td>\n",
              "      <td>0.726667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.520500</td>\n",
              "      <td>0.540756</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.481200</td>\n",
              "      <td>0.523099</td>\n",
              "      <td>0.773333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.446200</td>\n",
              "      <td>0.530244</td>\n",
              "      <td>0.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.352000</td>\n",
              "      <td>0.590401</td>\n",
              "      <td>0.760000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1200, training_loss=0.5595565827687582, metrics={'train_runtime': 460.9148, 'train_samples_per_second': 26.035, 'train_steps_per_second': 3.254, 'total_flos': 631466532864000.0, 'train_loss': 0.5595565827687582, 'epoch': 8.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate pruned model on validation set\n",
        "pruned_validation_results = trainer.evaluate(eval_dataset=validation_dataset)\n",
        "pruned_validation_accuracy = pruned_validation_results[\"eval_accuracy\"] * 100\n",
        "print(f\"Pruned Model Validation Accuracy: {pruned_validation_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate pruned model on test set\n",
        "pruned_test_results = trainer.evaluate(eval_dataset=small_test_dataset)\n",
        "pruned_test_accuracy = pruned_test_results[\"eval_accuracy\"] * 100\n",
        "print(f\"Pruned Model Test Accuracy: {pruned_test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "170Kwv3kFFgl",
        "outputId": "3ae9665a-221a-4d2f-84c1-2f51f0f1ecd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='101' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Model Validation Accuracy: 77.33%\n",
            "Pruned Model Test Accuracy: 73.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, dataset, batch_size=8, num_batches=10):\n",
        "    model.eval()\n",
        "    model.to('cuda')\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = {k: v.to('cuda') for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
        "            torch.cuda.synchronize()\n",
        "            start_time = time.time()\n",
        "            _ = model(**inputs)\n",
        "            torch.cuda.synchronize()\n",
        "            end_time = time.time()\n",
        "            times.append(end_time - start_time)\n",
        "\n",
        "    avg_time = sum(times) / len(times)\n",
        "    return avg_time\n",
        "\n",
        "# Measure inference time for pruned model\n",
        "pruned_model_avg_time = measure_inference_time(model, small_test_dataset)\n",
        "print(f\"Pruned Model Inference Time: {pruned_model_avg_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUzJYzjFHGu",
        "outputId": "7005150a-1d32-454b-f0e7-4e3f4e28fb12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Model Inference Time: 0.0557 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train original model\n",
        "original_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "original_trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(\"Training original model...\")\n",
        "original_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "etdqYyqGFWse",
        "outputId": "dc891b58-f46d-43f2-9229-4cc8a2622eab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-10-c611e4083a46>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  original_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training original model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='450' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 450/1500 03:46 < 08:51, 1.98 it/s, Epoch 3/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.557700</td>\n",
              "      <td>0.439133</td>\n",
              "      <td>0.826667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>0.480858</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.191300</td>\n",
              "      <td>0.525699</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=450, training_loss=0.3695038562350803, metrics={'train_runtime': 226.896, 'train_samples_per_second': 52.888, 'train_steps_per_second': 6.611, 'total_flos': 236799949824000.0, 'train_loss': 0.3695038562350803, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and train original model\n",
        "original_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "original_trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(\"Training original model...\")\n",
        "original_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "W7qFcIMXNKVy",
        "outputId": "7c45684c-a4d5-40ae-ea7c-6f25019a9599"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-11-c611e4083a46>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  original_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training original model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='450' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 450/1500 02:53 < 06:46, 2.58 it/s, Epoch 3/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.580800</td>\n",
              "      <td>0.394340</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.364900</td>\n",
              "      <td>0.577436</td>\n",
              "      <td>0.846667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.200800</td>\n",
              "      <td>0.567942</td>\n",
              "      <td>0.863333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=450, training_loss=0.3821798748440213, metrics={'train_runtime': 173.8256, 'train_samples_per_second': 69.035, 'train_steps_per_second': 8.629, 'total_flos': 236799949824000.0, 'train_loss': 0.3821798748440213, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time for original model\n",
        "original_model_avg_time = measure_inference_time(original_model, small_test_dataset)\n",
        "print(f\"Original Model Inference Time: {original_model_avg_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1GY-VM8SRzI",
        "outputId": "d9c5f561-eaf2-449b-8da1-518a19aaa944"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Inference Time: 0.0577 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total parameters for original model\n",
        "original_model_memory = sum(p.numel() for p in original_model.parameters())\n",
        "print(f\"Original Model Parameters: {original_model_memory:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28imQaCSTjp",
        "outputId": "abdf640f-898e-4226-f8e8-d357bcf2e2bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Parameters: 109,483,778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_test_results = original_trainer.evaluate(eval_dataset=small_test_dataset)\n",
        "original_model_accuracy_score = original_test_results[\"eval_accuracy\"] * 100\n",
        "print(f\"Original Model Test Accuracy: {original_model_accuracy_score:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qW00BWYVW3yG",
        "outputId": "67d92f74-85aa-4c75-a1b5-2e505cc3e2fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:50]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Test Accuracy: 83.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_file(\n",
        "    pruning_type,\n",
        "    dataset_used,\n",
        "    num_samples,\n",
        "    original_model_memory,\n",
        "    original_model_accuracy_score,\n",
        "    original_model_avg_time,\n",
        "    pruned_model_memory,\n",
        "    pruned_model_accuracy_score,\n",
        "    pruned_model_avg_time\n",
        "):\n",
        "    file_name = f\"{pruning_type}_pruning_summary.txt\"\n",
        "    content = (\n",
        "        f\"Pruning Method: {pruning_type.capitalize()} Pruning\\n\"\n",
        "        f\"Dataset Used: {dataset_used}\\n\"\n",
        "        f\"Number of Samples for Inference: {num_samples}\\n\\n\"\n",
        "        f\"Original Model Parameters: {original_model_memory:,}\\n\"\n",
        "        f\"Original Model Accuracy (%): {original_model_accuracy_score:.2f}\\n\"\n",
        "        f\"Original Model Inference Time (avg seconds): {original_model_avg_time:.4f}\\n\\n\"\n",
        "        f\"Pruned Model Parameters: {pruned_model_memory:,}\\n\"\n",
        "        f\"Pruned Model Accuracy (%): {pruned_model_accuracy_score:.2f}\\n\"\n",
        "        f\"Pruned Model Inference Time (avg seconds): {pruned_model_avg_time:.4f}\\n\"\n",
        "    )\n",
        "    file_path = os.path.join(os.getcwd(), file_name)\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.write(content)\n",
        "    print(f\"Summary saved to: {file_path}\")\n",
        "\n",
        "write_file(\n",
        "    pruning_type=\"structured\",\n",
        "    dataset_used=\"IMDB\",\n",
        "    num_samples=len(small_test_dataset),\n",
        "    original_model_memory=original_model_memory,\n",
        "    original_model_accuracy_score=original_model_accuracy_score,\n",
        "    original_model_avg_time=original_model_avg_time,\n",
        "    pruned_model_memory=pruned_model_memory,\n",
        "    pruned_model_accuracy_score=pruned_test_accuracy,\n",
        "    pruned_model_avg_time=pruned_model_avg_time\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2T3Gx9pFYbO",
        "outputId": "4fb617f0-f8d1-4f50-babf-07e7f64f7a08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary saved to: /content/structured_pruning_summary.txt\n"
          ]
        }
      ]
    }
  ]
}